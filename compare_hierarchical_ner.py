"""
Comparison script to demonstrate the Hierarchical NER system
Shows performance with and without the large model escalation
"""

import time
from hierarchical_ner import HierarchicalNER
from hierarchical_ner_config import (
    HierarchicalNERConfig,
    ModelPerformanceTracker,
    get_fast_config,
    get_accurate_config
)
from tabulate import tabulate


def compare_predictions(text: str, ner_system: HierarchicalNER):
    """
    Compare predictions with different uncertainty thresholds.
    """
    print("\n" + "üî∑" * 35)
    print(f"INPUT TEXT: {text}")
    print("üî∑" * 35)
    
    # Test with different thresholds
    thresholds = [0.2, 0.4, 0.6]
    results = []
    
    for threshold in thresholds:
        ner_system.uncertainty_threshold = threshold
        start_time = time.time()
        
        entities = ner_system.predict(text, verbose=False)
        
        elapsed = time.time() - start_time
        
        # Check if large model was used
        _, uncertainty = ner_system._run_small_models(text)
        used_large = uncertainty > threshold
        
        results.append({
            'Threshold': threshold,
            'Uncertainty': f"{uncertainty:.3f}",
            'Large Model': 'Yes' if used_large else 'No',
            'Entities Found': len(entities),
            'Time (s)': f"{elapsed:.2f}"
        })
    
    # Print comparison table
    print("\n" + "=" * 80)
    print("THRESHOLD COMPARISON")
    print("=" * 80)
    table = tabulate(results, headers='keys', tablefmt='grid')
    print(table)
    
    # Show final predictions
    print("\n" + "=" * 80)
    print("ENTITIES DETECTED (with optimal threshold)")
    print("=" * 80)
    
    # Use middle threshold for final result
    ner_system.uncertainty_threshold = 0.3
    final_entities = ner_system.predict(text, verbose=False)
    
    if final_entities:
        entity_table = []
        for e in final_entities:
            entity_table.append({
                'Entity': e['word'],
                'Type': e['entity_group'],
                'Confidence': f"{e['score']:.3f}",
                'Votes': e['total_votes']
            })
        print(tabulate(entity_table, headers='keys', tablefmt='grid'))
    else:
        print("No entities detected.")


def batch_evaluation(test_cases: list):
    """
    Evaluate the system on multiple test cases.
    """
    print("\n" + "üåü" * 35)
    print("BATCH EVALUATION - HIERARCHICAL NER SYSTEM")
    print("üåü" * 35)
    
    ner_system = HierarchicalNER(uncertainty_threshold=0.3, use_gpu=True)
    tracker = ModelPerformanceTracker()
    
    results = []
    
    for i, (text, expected_difficulty) in enumerate(test_cases, 1):
        print(f"\n--- Test Case {i}/{len(test_cases)} ({expected_difficulty} difficulty) ---")
        print(f"Text: {text[:80]}..." if len(text) > 80 else f"Text: {text}")
        
        start_time = time.time()
        
        # Get predictions
        _, uncertainty = ner_system._run_small_models(text)
        entities = ner_system.predict(text, verbose=False)
        
        elapsed = time.time() - start_time
        used_large = uncertainty > ner_system.uncertainty_threshold
        
        # Record stats
        tracker.record_prediction(
            uncertainty=uncertainty,
            used_large_model=used_large,
            num_entities=len(entities),
            processing_time=elapsed
        )
        
        results.append({
            'Case': i,
            'Difficulty': expected_difficulty,
            'Uncertainty': f"{uncertainty:.3f}",
            'Large Model': '‚úì' if used_large else '‚úó',
            'Entities': len(entities),
            'Time (s)': f"{elapsed:.2f}"
        })
        
        print(f"  ‚Üí Uncertainty: {uncertainty:.3f}, Entities: {len(entities)}, Time: {elapsed:.2f}s")
    
    # Print summary table
    print("\n" + "=" * 90)
    print("EVALUATION RESULTS")
    print("=" * 90)
    table = tabulate(results, headers='keys', tablefmt='grid')
    print(table)
    
    # Print performance summary
    tracker.print_summary()
    
    return tracker


def demonstrate_voting_mechanism():
    """
    Demonstrate how the weighted voting works with detailed breakdown.
    """
    print("\n" + "üó≥Ô∏è " * 35)
    print("WEIGHTED VOTING MECHANISM DEMONSTRATION")
    print("üó≥Ô∏è " * 35)
    
    # Use a text that will likely trigger disagreement
    text = "Jordan loves to play basketball in Chicago."
    
    ner_system = HierarchicalNER(uncertainty_threshold=0.15, use_gpu=True)
    
    print(f"\nTest Text: {text}")
    print("\nThis text is ambiguous:")
    print("  - 'Jordan' could be a person (Michael Jordan) or location (country)")
    print("  - 'Chicago' is clearly a location")
    print("\nLet's see how the models vote...\n")
    
    # Run with detailed output
    entities = ner_system.predict(text, verbose=True)
    
    print("\n" + "=" * 80)
    print("VOTING ANALYSIS")
    print("=" * 80)
    
    for entity in entities:
        print(f"\n'{entity['word']}' (Position {entity['start']}-{entity['end']}):")
        print(f"  Final Type: {entity['entity_group']}")
        print(f"  Total Votes: {entity['total_votes']}")
        print(f"  Vote Breakdown:")
        for ent_type, votes in entity['vote_details'].items():
            print(f"    - {ent_type}: {votes} vote(s)")
        print(f"  Winner: {entity['entity_group']} (most votes)")


def main():
    """Main comparison demonstration."""
    
    # Test cases with varying difficulty
    test_cases = [
        ("Apple Inc. announced the new iPhone at their headquarters in Cupertino.", "Easy"),
        ("The President met with Angela Merkel in Berlin yesterday.", "Medium"),
        ("Jordan scored 23 points while visiting the Jordan River valley.", "Hard"),
        ("Amazon is cutting down the Amazon rainforest for profit.", "Hard"),
        ("Paris Hilton visited Paris, France for a fashion show.", "Hard"),
        ("Google's CEO Sundar Pichai announced new AI features.", "Easy"),
        ("The Nile flows through Egypt and Sudan in Northeast Africa.", "Medium"),
        ("Washington addressed the nation from Washington D.C.", "Medium"),
    ]
    
    # 1. Single text comparison with different thresholds
    print("\n" + "=" * 90)
    print("DEMONSTRATION 1: Threshold Sensitivity Analysis")
    print("=" * 90)
    
    ner_system = HierarchicalNER(uncertainty_threshold=0.3, use_gpu=True)
    compare_predictions(test_cases[2][0], ner_system)
    
    # 2. Batch evaluation
    print("\n\n" + "=" * 90)
    print("DEMONSTRATION 2: Batch Evaluation")
    print("=" * 90)
    
    tracker = batch_evaluation(test_cases)
    
    # 3. Voting mechanism demonstration
    print("\n\n" + "=" * 90)
    print("DEMONSTRATION 3: Voting Mechanism Deep Dive")
    print("=" * 90)
    
    demonstrate_voting_mechanism()
    
    # 4. Configuration comparison
    print("\n\n" + "=" * 90)
    print("DEMONSTRATION 4: Configuration Comparison")
    print("=" * 90)
    
    configs = {
        "Fast (High Threshold)": get_fast_config(),
        "Accurate (Low Threshold)": get_accurate_config()
    }
    
    test_text = "Microsoft CEO Satya Nadella spoke at the conference in Seattle."
    
    for config_name, config in configs.items():
        print(f"\n{'='*80}")
        print(f"Configuration: {config_name}")
        print(f"{'='*80}")
        print(config.summary())
    
    # Final summary
    print("\n\n" + "üéØ" * 35)
    print("KEY INSIGHTS")
    print("üéØ" * 35)
    
    print("""
    1. THRESHOLD SELECTION:
       - Lower threshold (0.2): More accurate but slower (uses large model more)
       - Higher threshold (0.5): Faster but may miss difficult cases
       - Sweet spot (0.3): Good balance for most applications
    
    2. VOTING MECHANISM:
       - Small models (BERT, DistilBERT, RoBERTa): 1 vote each
       - Large model: 2 votes (counts double)
       - Weighted voting resolves disagreements effectively
    
    3. WHEN LARGE MODEL HELPS:
       - Ambiguous entity names (Jordan = person or place?)
       - Multiple entities with same name (Paris the person vs. Paris the city)
       - Complex sentences with many entities
    
    4. EFFICIENCY:
       - Most texts can be handled by small models alone
       - Large model only loads when needed (saves memory and time)
       - Typical large model usage rate: 20-40% of cases
    """)
    
    print("\n" + "‚úÖ" * 35)
    print("DEMONSTRATION COMPLETE")
    print("‚úÖ" * 35)


if __name__ == "__main__":
    main()

