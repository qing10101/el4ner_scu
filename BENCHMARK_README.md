# Benchmark Scripts for Hierarchical NER

## üìä Overview

This directory contains comprehensive benchmark scripts to compare the Hierarchical NER system against:
- **3 Single Models** (BERT, DistilBERT, RoBERTa) operating independently
- **1 Large Model** (XLM-RoBERTa-large) operating independently

## üöÄ Quick Start

### Option 1: Quick Benchmark (5 minutes)

```bash
python quick_benchmark.py
```

This runs a simplified benchmark on 3 default test texts.

**Customize:**
```bash
# Use more texts
python quick_benchmark.py --num-texts 10

# Use custom text file
python quick_benchmark.py --texts-file my_texts.json

# Force CPU usage
python quick_benchmark.py --cpu
```

### Option 2: Comprehensive Benchmark (30+ minutes)

```bash
python benchmark_hierarchical_ner.py
```

This runs a detailed benchmark with:
- Execution time measurements
- Memory usage tracking
- Entity overlap/agreement analysis
- Detailed statistics
- Results saved to JSON

**Options:**
```bash
# Quick test (3 texts)
python benchmark_hierarchical_ner.py --quick

# Use 20 texts
python benchmark_hierarchical_ner.py --num-texts 20

# Custom text file
python benchmark_hierarchical_ner.py --texts-file data/test_texts.json

# Custom output file
python benchmark_hierarchical_ner.py --output my_results.json

# Force CPU
python benchmark_hierarchical_ner.py --cpu
```

## üìã What Gets Measured

### Performance Metrics

1. **Execution Time**
   - Average time per text
   - Total processing time
   - Speed (texts per second)
   - Min/max/std deviation

2. **Entity Detection**
   - Average entities found per text
   - Entity count distribution
   - Entity overlap between approaches

3. **Agreement Metrics** (Hierarchical vs Others)
   - Jaccard similarity
   - Precision
   - Recall
   - F1 score

4. **Efficiency Metrics** (Hierarchical-specific)
   - Large model usage rate
   - Time savings vs always-large
   - Memory efficiency

5. **Memory Usage**
   - Peak memory per approach
   - Average memory consumption

## üìä Example Output

### Quick Benchmark Output

```
======================================================================
RESULTS SUMMARY
======================================================================

+-------------+---------------+---------------+--------------+--------------+
| Approach    | Avg Time (s)  | Total Time (s) | Avg Entities | Speed (text/s) |
+=============+===============+===============+==============+==============+
| BERT        | 0.234         | 1.17           | 3.2          | 4.27          |
| DistilBERT  | 0.189         | 0.95           | 2.8          | 5.29          |
| RoBERTa     | 0.312         | 1.56           | 3.5          | 3.21          |
| Large       | 0.856         | 4.28           | 3.8          | 1.17          |
| Hierarchical| 0.421         | 2.11           | 3.6          | 2.38          |
+-------------+---------------+---------------+--------------+--------------+

======================================================================
COMPARISON WITH HIERARCHICAL
======================================================================

+------------------+-------------+---------------+
| vs Hierarchical | Speed Ratio | Faster/Slower |
+==================+=============+===============+
| BERT             | 0.56x       | Faster        |
| DistilBERT       | 0.45x       | Faster        |
| RoBERTa          | 0.74x       | Faster        |
| Large            | 2.03x       | Slower        |
+------------------+-------------+---------------+

======================================================================
EFFICIENCY ANALYSIS
======================================================================

Hierarchical vs Always-Large Model:
  Time per text: 0.421s vs 0.856s
  Speed improvement: 2.03x faster
  Time savings: 50.8%
  Large model used: 3/10 times (30.0%)
```

### Comprehensive Benchmark Output

The comprehensive benchmark includes additional:
- Detailed entity overlap matrices
- Memory usage statistics
- Per-text breakdowns
- JSON export with all data

## üìÅ Input Format

### Custom Text File (JSON)

Create a JSON file with texts:

```json
[
  "Apple Inc. announced new products in California.",
  "Microsoft CEO Satya Nadella spoke in Seattle.",
  "Jordan scored while visiting Jordan River."
]
```

Or with metadata:

```json
{
  "texts": [
    "Apple Inc. announced new products in California.",
    "Microsoft CEO Satya Nadella spoke in Seattle."
  ],
  "metadata": {
    "source": "test_data",
    "date": "2024-01-01"
  }
}
```

## üìà Output Files

### Comprehensive Benchmark Output

The comprehensive benchmark saves a detailed JSON file:

```json
{
  "summary": {
    "total_texts": 10,
    "approaches": {
      "hierarchical": {
        "avg_time": 0.421,
        "avg_entities": 3.6,
        "large_model_usage_rate": 0.3
      }
    },
    "comparisons": {
      "hierarchical_vs_large": {
        "avg_f1": 0.92
      }
    }
  },
  "detailed_results": [
    {
      "text_id": 1,
      "approaches": {
        "hierarchical": {
          "entities": [...],
          "time": 0.421
        }
      }
    }
  ]
}
```

## üéØ Use Cases

### 1. Quick Performance Check

```bash
python quick_benchmark.py --num-texts 5
```

**When to use:**
- Quick validation after changes
- Testing on a few texts
- Development/debugging

### 2. Comprehensive Evaluation

```bash
python benchmark_hierarchical_ner.py --num-texts 50 --output results.json
```

**When to use:**
- Final evaluation before deployment
- Research/paper experiments
- Performance optimization analysis
- Need detailed statistics

### 3. Custom Dataset Evaluation

```bash
python benchmark_hierarchical_ner.py --texts-file my_dataset.json
```

**When to use:**
- Evaluating on your specific domain
- Comparing approaches on your data
- Production performance testing

## üìä Interpreting Results

### Speed Comparison

- **< 1.0x**: Faster than hierarchical (but less accurate)
- **~ 1.0x**: Similar speed
- **> 1.0x**: Slower than hierarchical
- **> 2.0x**: Much slower (consider using hierarchical)

### Entity Agreement

- **F1 > 0.9**: Very similar predictions
- **F1 0.7-0.9**: Generally similar with some differences
- **F1 < 0.7**: Significant differences in predictions

### Large Model Usage

- **< 20%**: Most texts handled by small models (fast)
- **20-40%**: Good balance (recommended)
- **> 50%**: Large model used often (consider lowering threshold)

## üîß Troubleshooting

### Out of Memory

```bash
# Use CPU (slower but less memory)
python benchmark_hierarchical_ner.py --cpu

# Use fewer texts
python benchmark_hierarchical_ner.py --num-texts 3
```

### Slow Execution

```bash
# Use quick benchmark
python quick_benchmark.py

# Or reduce number of texts
python benchmark_hierarchical_ner.py --num-texts 5
```

### Model Download Issues

First run will download models (5-10 minutes). Ensure:
- Stable internet connection
- Hugging Face token if needed (for gated models)
- Enough disk space (~5GB for all models)

## üìà Expected Results

Based on testing:

| Approach | Avg Time | Entities | Speed vs Hierarchical |
|----------|----------|----------|----------------------|
| BERT | 0.2-0.3s | 3.0-3.5 | 0.5x (faster) |
| DistilBERT | 0.15-0.25s | 2.5-3.0 | 0.4x (faster) |
| RoBERTa | 0.3-0.4s | 3.2-3.8 | 0.7x (faster) |
| Large | 0.8-1.0s | 3.5-4.0 | 2.0x (slower) |
| **Hierarchical** | **0.4-0.5s** | **3.4-3.8** | **1.0x (baseline)** |

**Key Insight**: Hierarchical is ~2x faster than always-large while finding similar number of entities!

## üéì Advanced Usage

### Compare Specific Approaches

Edit the benchmark script to test only specific models:

```python
# In benchmark_hierarchical_ner.py, modify:
self.small_models_config = {
    'bert': 'dslim/bert-base-NER',  # Keep only what you need
}
```

### Custom Metrics

Add your own metrics to the benchmark:

```python
def custom_metric(self, entities):
    # Your custom calculation
    return result
```

### Batch Processing with Progress

```python
from tqdm import tqdm

for text in tqdm(texts):
    result = benchmark.benchmark_single_text(text)
```

## üìö Related Documentation

- **Main Documentation**: `HIERARCHICAL_NER_README.md`
- **Quick Start**: `QUICKSTART.md`
- **Implementation**: `IMPLEMENTATION_SUMMARY.md`
- **Comparison Table**: `COMPARISON_TABLE.md`

## üí° Tips

1. **Start with quick benchmark** to verify setup
2. **Use comprehensive for final evaluation**
3. **Save results** for later comparison
4. **Test on your domain-specific texts** for realistic results
5. **Compare entity overlap** to understand quality differences

## üéØ Benchmark Checklist

Before running benchmarks:

- [ ] Install all dependencies (`pip install -r requirements_hierarchical.txt`)
- [ ] Have enough disk space (~5GB for models)
- [ ] GPU available (optional but recommended)
- [ ] Test texts prepared (or use defaults)
- [ ] Output directory writable (for JSON results)

---

**Happy Benchmarking! üìäüöÄ**

